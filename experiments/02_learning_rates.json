{
  "description": "Learning rate exploration with different optimizers and schedulers",
  "experiment_name": "lr_exploration",
  "parameters": {
    "lr": [0.0001, 0.0003, 0.001, 0.003, 0.01],
    "optimizer": ["adam", "adamw", "sgd"],
    "scheduler": [null, "cosine", "reduce_on_plateau"]
  },
  "fixed_parameters": {
    "model_name": "efficientnet_b0",
    "batch_size": 32,
    "loss": "ce",
    "max_epochs": 30,
    "early_stopping": 10,
    "img_size": 224,
    "num_workers": 4,
    "pretrained": true
  }
}